{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing and Preprocessing - Women's March 2017 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook trata da parte de leitura dos dados dos tweets de arquivos `.json` e pré-processamento das informações contidas nestes arquivos.\n",
    "\n",
    "Quando a API do Twitter retorna dados de tweets recuperados através dela, eles são entregues em um arquivo JSON com vários dados e metadados que compõe um tweet. Muitos desses dados contidos no JSON são desnecessários para a análise que temos como objetivo, por isso, os trechos de código abaixo tratam as informações necessárias para a análise e descartam as demais.\n",
    "\n",
    "Para esse trabalho utilizamos bibliotecas específicas além do que a linguagem provém:\n",
    "\n",
    "- **`os`**: lida com detalhes específicos ao sistema operacional da máquina rodando a análise, independente de qual sistemaoperacional seja;\n",
    "- **`re`**: lida com expressões regulares pra reconhecimento de padrões em textos;\n",
    "- **`glob`**: lida com padrão de arquivos em sistemas Unix;\n",
    "- **`pandas`**: toolkit para análise de dados;\n",
    "- **`json_normalize`**: um sub-módulo do pandas para tratamento de arquivos e dados JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com auxílio da biblioteca **`re`**, foram criadas funções para converter algumas informações presentes no texto do tweet para informações simples e concisas para análise posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_hashtags(text):\n",
    "    s = re.findall('(?:^|\\s)[＃#]{1}(\\w+)', text)\n",
    "    return s if len(s) > 0 else ''\n",
    "\n",
    "def get_mentions(text):\n",
    "    s = re.findall('(?:^|\\s?|\\.?)[＠@]{1}([^\\s#<>[\\]|{}:;,.\\(\\)=+]+)', text)\n",
    "    return s if len(s) > 0 else ''\n",
    "\n",
    "def get_source(text):\n",
    "    s = re.findall('<a\\s+?href=\\\"[^\\\"]*\\\"\\s+?rel=\\\"[^\\\"]*\\\">([^<>]+)<\\/a>', text)\n",
    "    return s[0] if len(s) > 0 else ''\n",
    "\n",
    "def get_urls(text):\n",
    "    s = re.findall('http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', text)\n",
    "    return s[0] if len(s) > 0 else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "filenames = glob.glob(os.path.join(path, '*.json'))\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido ao enorme número de dados que foram recuperados da API (11.249.944 tweets), o pré-processamento foi feito em blocos. \n",
    "\n",
    "Na importação dos dados da API, os tweets foram divididos em 16 arquivos, e na importação dos dados esses 16 arquivos foram lidos, pré-processados, e outros 16 arquivos foram criados com os dados prontos para a análise, que geraram 9.170.486 instâncias.\n",
    "\n",
    "Os dados para análise se resumem em:\n",
    "\n",
    "- id do tweet\n",
    "- data e hora de criação do tweet\n",
    "- dispositivo fonte\n",
    "- texto do tweet\n",
    "- hashtags presentes no tweet\n",
    "- menções à usuários\n",
    "- urls presentes no tweet\n",
    "- número de vezes que o tweet foi favoritado\n",
    "- número de vezes que o tweet foi retweetado\n",
    "- localização do usuário\n",
    "- nome do usuário\n",
    "- username\n",
    "- quantos seguidores o usuário tem\n",
    "- se é um usuário verificado ou não\n",
    "\n",
    "Após o pré-processamento dos tweets, onde foram escolhidos apenas 14 características para observação, conseguimos reduzir a nossa amostra de um arquivo de cerca de 96GB iniciais para um arquivo de 4,8GB, descartando apenas os metadados desnecessários e os tweets de idioma diferente do inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in filenames:\n",
    "    json_reader = pd.read_json(file, lines=True, chunksize=2048)\n",
    "\n",
    "    wm_data = pd.DataFrame()\n",
    "\n",
    "    for chunk in json_reader:\n",
    "        not_truncated = chunk[chunk['truncated'] == False]\n",
    "        only_english = not_truncated[not_truncated['lang'] == 'en'].reset_index()\n",
    "\n",
    "        only_english['hashtags'] = only_english['text'].apply(get_hashtags)\n",
    "        only_english['mentions'] = only_english['text'].apply(get_mentions)\n",
    "        only_english['urls'] = only_english['text'].apply(get_urls)\n",
    "        only_english['source'] = only_english['source'].apply(get_source)\n",
    "        user_df = json_normalize(only_english['user'])\n",
    "\n",
    "        # Selecting only few columns\n",
    "        tweet_df = only_english[['id_str', 'created_at', 'source', 'text', 'hashtags', 'mentions', \\\n",
    "                                 'urls', 'favorite_count', 'retweet_count']]\n",
    "        user_df = user_df[['location', 'name', 'screen_name', 'followers_count', 'verified']]\n",
    "\n",
    "        frames = [tweet_df, user_df]\n",
    "\n",
    "        df = pd.concat(frames, axis=1)\n",
    "\n",
    "        wm_data.append(df)\n",
    "        \n",
    "    fp = file[2:]\n",
    "    filepath = '../data/clean_{}'.format(fp)    \n",
    "        \n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(wm_data.to_json(orient='records', lines=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
